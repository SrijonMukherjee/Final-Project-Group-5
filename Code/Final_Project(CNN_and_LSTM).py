# -*- coding: utf-8 -*-
"""Final_Project(CNN_and_LSTM).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UjP41ouzrPZW_8dyXIY62YW18pchXJ6y
"""

from google.colab import files
files.upload()  #this will prompt you to upload the kaggle.json

# Importing Required Libraries
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords

from numpy import array
import tensorflow
from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers.core import Activation, Dropout, Dense
from keras.layers import Flatten
from keras.layers import GlobalMaxPooling1D
from keras.layers.embeddings import Embedding
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt

# Import the dataset
movie_reviews_labeled = pd.read_csv("labeledTrainData.tsv", header=0, delimiter="\t", quoting=3)

movie_reviews_unlabeled = pd.read_csv("unlabeledTrainData.tsv", header=0, delimiter="\t", quoting=3)

test_data = pd.read_csv("testData.tsv", header=0, delimiter="\t", quoting=3)

# Convert .tsv file to .csv file and read the csv file 
movie_reviews_labeled.to_csv('movie_reviews_labeled.csv')
movie_reviews = pd.read_csv('movie_reviews_labeled.csv')

# Remove quotations from string and print first 5 rows of the dataset
movie_reviews['review'] = movie_reviews['review'].str.strip('" "')
movie_reviews.head()

# Take a look at any one of the reviews
movie_reviews['review'][0]

# See the size of positive and negative sentiments in this dataset
movie_reviews.shape

# Take a text string as a parameter
# Performs preprocessing on the string to remove special chracters from the string

def preprocess_text(sen):
    # Removing html tags
    sentence = remove_tags(sen)

    # Remove punctuations and numbers
    sentence = re.sub('[^a-zA-Z]', ' ', sentence)

    # Single character removal
    sentence = re.sub(r"\s+[a-zA-Z]\s+", ' ', sentence)

    # Removing multiple spaces
    sentence = re.sub(r'\s+', ' ', sentence)

    return sentence


TAG_RE = re.compile(r'<[^>]+>')

def remove_tags(text):
    return TAG_RE.sub('', text)

# Preprocess our reviews and will store them in a new list as shown below
X = []
sentences = list(movie_reviews['review'])
for sen in sentences:
    X.append(preprocess_text(sen))

X[0]

y = movie_reviews['sentiment']

# Divide the dataset into 80% for training set and 20% for testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

# Prepare the embedding layer
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

# Find the vocabulary size and then perform padding on both train and test set
# Adding 1 because of reserved 0 index
vocab_size = len(tokenizer.word_index) + 1

maxlen = 100

X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)
X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)

!wget nlp.stanford.edu/data/glove.6B.zip

#unzipping the zip files and deleting the zip files
!unzip \*.zip  && rm *.zip

# Load the GloVe word embeddings
# Create a dictionary that will contain words as keys and their corresponding embedding list as values.
from numpy import array
from numpy import asarray
from numpy import zeros

embeddings_dictionary = dict()
glove_file = open('glove.6B.100d.txt', encoding="utf8")

for line in glove_file:
    records = line.split()
    word = records[0]
    vector_dimensions = asarray(records[1:], dtype='float32')
    embeddings_dictionary [word] = vector_dimensions
glove_file.close()

# Create an embedding matrix where each row number will correspond to the index of the word in the corpus
embedding_matrix = zeros((vocab_size, 100))
for word, index in tokenizer.word_index.items():
    embedding_vector = embeddings_dictionary.get(word)
    if embedding_vector is not None:
        embedding_matrix[index] = embedding_vector

# Create a simple convolutional neural network with 1 convolutional layer and 1 pooling layer
from keras.layers.convolutional import Conv1D
model = Sequential()

embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)

model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))
model.add(GlobalMaxPooling1D())
model.add(Dense(1, activation='sigmoid'))
#opt = tensorflow.keras.optimizers.Adam(learning_rate = 0.01)
model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer='adam')

print(model.summary())

# Train our model and evaluate it on the training set
history = model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.33)

score = model.evaluate(X_test, y_test, verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

"""EVAlUATING CNN:"""

pred_cnn = model.predict(X_test)
pred_cnn = (pred_cnn>0.5).astype(int)

import seaborn as sns
conf_mat = confusion_matrix(y_test, pred_cnn)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=['Negative','Positive'], yticklabels = ['Negative','Positive'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig('cnn_cm', dpi = 720)
plt.show()

report = metrics.classification_report(y_test, pred_cnn)

from sklearn.metrics import roc_curve, roc_auc_score
pred_cnn = model.predict(X_test)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(y_test, pred_cnn)

from sklearn.metrics import auc

plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label='Area Under Curve (AUC) =  {:.3f}'.format(auc(fpr,tpr)))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve for Predicting Sentiments')
plt.legend(loc='best')
plt.savefig('auc_cnn.jpg', dpi = 720)
plt.show()

"""Configuring Learning Rate and Batch Size:

Low Batch Size:
"""



from tensorflow.keras.callbacks import LearningRateScheduler
initial_learning_rate = 0.01
epochs = 5
decay = initial_learning_rate / epochs
def lr_time_based_decay(epoch, lr):
  lrate = lr * 1 / (1 + decay * epoch)
  return lrate

lrate = LearningRateScheduler(lr_time_based_decay, verbose = 1)

# Create a Bidirectional LSTM with low learning rate

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, SpatialDropout1D
from tensorflow.keras.layers import Embedding, Flatten
from tensorflow.keras.layers import MaxPooling1D, Dropout, Activation, Conv1D

model = Sequential()
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)
model.add(SpatialDropout1D(0.3))
model.add(Bidirectional(LSTM(100, dropout=0.3, recurrent_dropout=0.2)))
model.add(Dense(1, activation='sigmoid'))
opt = tensorflow.keras.optimizers.Adam(learning_rate = 0.001)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

print(model.summary())

history = model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1, validation_split=0.33)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])



"""EVALUATING LSTM:"""

pred_lstm = model.predict(X_test)

pred_label_lstm = (pred_lstm>0.5).astype(int)

report = metrics.classification_report(y_test, pred_label_lstm)
print(report)

import seaborn as sns
conf_mat= confusion_matrix(y_test, pred_label_lstm)
fig, ax = plt.subplots(figsize=(10,10))
sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=['Negative','Positive'], yticklabels = ['Negative','Positive'])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig('lstm_cm.jpg', dpi = 720)
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
pred_lstm = model.predict(X_test)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(y_test, pred_lstm)

from sklearn.metrics import auc

plt.figure(1)
plt.plot([0, 1], [0, 1], 'k--')
plt.plot(fpr, tpr, label='Area Under Curve (AUC) =  {:.3f}'.format(auc(fpr,tpr)))
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve for Predicting Positive Sentiments')
plt.legend(loc='best')
plt.savefig('auc_lstm.jpg', dpi = 720)
plt.show()



plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

"""High Batch Size with Higher Learning Rate:"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, SpatialDropout1D
from tensorflow.keras.layers import Embedding, Flatten
from tensorflow.keras.layers import MaxPooling1D, Dropout, Activation, Conv1D

model = Sequential()
embedding_layer = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen , trainable=False)
model.add(embedding_layer)
model.add(SpatialDropout1D(0.3))
model.add(LSTM(128,recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
opt = tensorflow.keras.optimizers.Adam(learning_rate = 0.01)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

print(model.summary())

from tensorflow.keras.callbacks import LearningRateScheduler
# Train our model and evaluate it on the training set
history = model.fit(X_train, y_train, batch_size=128, epochs=15, verbose=1, validation_split=0.33)

score = model.evaluate(X_test, y_test, verbose=1)

print("Test Score:", score[0])
print("Test Accuracy:", score[1])

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'], loc = 'upper left')
plt.show()